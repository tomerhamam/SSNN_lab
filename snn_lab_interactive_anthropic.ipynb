{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Supervised Neural Networks: An Interactive Lab\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this lab, you will:\n",
    "- Understand the core principles of self-supervised learning (SSL)\n",
    "- Implement pretext tasks for vision and time-series data\n",
    "- Evaluate representation quality through transfer learning\n",
    "- Compare generative vs. discriminative SSL approaches\n",
    "- Build intuition about when and why SSL works\n",
    "\n",
    "## üìö Prerequisites\n",
    "- Basic understanding of neural networks and backpropagation\n",
    "- Familiarity with NumPy and Python\n",
    "- Linear algebra fundamentals (matrix multiplication, derivatives)\n",
    "\n",
    "## üîó Recommended Reading\n",
    "Before starting, consider reviewing:\n",
    "- [A Survey on Self-supervised Learning](https://arxiv.org/abs/2301.05712)\n",
    "- [Representation Learning: A Review and New Perspectives](https://arxiv.org/abs/1206.5538)\n",
    "- [Self-supervised Visual Feature Learning with Deep Neural Networks](https://arxiv.org/abs/1712.05577)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install Required Package for Anthropic API\n",
    "\n",
    "First, let's install the Anthropic Python SDK if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Anthropic SDK (uncomment if needed)\n",
    "# !pip install anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 1: Introduction to Self-Supervised Learning\n",
    "\n",
    "Self-supervised learning (SSL) is a paradigm where models learn representations from unlabeled data by solving **pretext tasks**. The model generates its own supervision signal from the data structure itself.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**Two Main Families of SSL:**\n",
    "1. **Generative/Predictive Methods**: Reconstruct or predict part of the input\n",
    "   - Examples: Autoencoders, masked language modeling (BERT), image inpainting\n",
    "   - Learns by minimizing reconstruction error\n",
    "\n",
    "2. **Discriminative/Contrastive Methods**: Learn to distinguish between different views\n",
    "   - Examples: SimCLR, MoCo, SwAV\n",
    "   - Learns by pulling positive pairs together, pushing negatives apart\n",
    "\n",
    "### ü§î Critical Thinking Question 1\n",
    "**Why might SSL be particularly valuable in domains like medical imaging or astronomy?**\n",
    "\n",
    "*Think about data availability, labeling costs, and domain expertise requirements.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment Module: Open-Ended Questions with Anthropic Claude\n",
    "\n",
    "This module provides automatic evaluation of open-ended questions using Claude API.\n",
    "\n",
    "### Setting up your API Key\n",
    "\n",
    "1. Get your API key from [Anthropic Console](https://console.anthropic.com/)\n",
    "2. Set it as an environment variable:\n",
    "   ```bash\n",
    "   export ANTHROPIC_API_KEY=\"your-api-key-here\"\n",
    "   ```\n",
    "   Or set it directly in the notebook (see next cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Optional\n",
    "import json\n",
    "\n",
    "# Option 1: Set API key directly (replace with your actual key)\n",
    "# os.environ['ANTHROPIC_API_KEY'] = 'your-api-key-here'\n",
    "\n",
    "# Option 2: Load from environment (if already set in ~/.bashrc)\n",
    "api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "if api_key:\n",
    "    print(\"‚úÖ Anthropic API key found in environment\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No API key found. Set ANTHROPIC_API_KEY environment variable for automatic evaluation.\")\n",
    "    print(\"   You can still use manual evaluation with provided rubrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenEndedAssessment:\n",
    "    \"\"\"Handle open-ended questions with Claude AI verification.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: Optional[str] = None):\n",
    "        self.api_key = api_key or os.getenv('ANTHROPIC_API_KEY')\n",
    "        self.questions = self._load_questions()\n",
    "        \n",
    "        # Only import anthropic if we have an API key\n",
    "        if self.api_key:\n",
    "            try:\n",
    "                import anthropic\n",
    "                self.client = anthropic.Anthropic(api_key=self.api_key)\n",
    "                print(\"‚úÖ Anthropic client initialized successfully\")\n",
    "            except ImportError:\n",
    "                print(\"‚ö†Ô∏è Please install anthropic: pip install anthropic\")\n",
    "                self.client = None\n",
    "        else:\n",
    "            self.client = None\n",
    "    \n",
    "    def _load_questions(self) -> List[Dict]:\n",
    "        \"\"\"Load assessment questions.\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"id\": \"q1\",\n",
    "                \"question\": \"Explain why rotation prediction is an effective pretext task for learning visual features. What properties of the task make it useful?\",\n",
    "                \"rubric\": [\n",
    "                    \"Mentions that rotation is a geometric transformation\",\n",
    "                    \"Notes that it requires understanding object structure\",\n",
    "                    \"Explains that labels are free (self-supervised)\",\n",
    "                    \"Discusses invariance/equivariance properties\"\n",
    "                ],\n",
    "                \"sample_answer\": \"Rotation prediction works because it forces the network to understand spatial structure and object geometry. The task requires recognizing features regardless of orientation, learning rotation-equivariant representations. Labels are automatically generated without human annotation.\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"q2\",\n",
    "                \"question\": \"Compare and contrast autoencoders with contrastive learning methods. When would you choose one over the other?\",\n",
    "                \"rubric\": [\n",
    "                    \"Identifies autoencoders as generative/reconstructive\",\n",
    "                    \"Identifies contrastive as discriminative\",\n",
    "                    \"Mentions computational efficiency differences\",\n",
    "                    \"Discusses use cases for each\"\n",
    "                ],\n",
    "                \"sample_answer\": \"Autoencoders learn by reconstruction, capturing all input details including noise. Contrastive methods learn by comparing samples, focusing on discriminative features. Autoencoders are simpler but may learn trivial solutions. Contrastive methods are more robust but require careful augmentation design.\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"q3\",\n",
    "                \"question\": \"Design a novel pretext task for learning representations from text data. Explain your reasoning.\",\n",
    "                \"rubric\": [\n",
    "                    \"Proposes a specific, implementable task\",\n",
    "                    \"Explains how labels are generated automatically\",\n",
    "                    \"Justifies why the task would learn useful features\",\n",
    "                    \"Considers computational feasibility\"\n",
    "                ],\n",
    "                \"sample_answer\": \"One novel task could be 'sentence ordering': given shuffled sentences from a paragraph, predict the correct order. This requires understanding discourse structure, temporal relationships, and causal dependencies. Labels come from the original ordering, making it fully self-supervised.\"\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def evaluate_answer(self, question_id: str, user_answer: str) -> Dict:\n",
    "        \"\"\"Evaluate user answer using Claude API.\"\"\"\n",
    "        question_data = next((q for q in self.questions if q['id'] == question_id), None)\n",
    "        if not question_data:\n",
    "            return {\"error\": \"Question not found\"}\n",
    "        \n",
    "        if not self.client:\n",
    "            return self._manual_evaluation(question_data, user_answer)\n",
    "        \n",
    "        # Prepare evaluation prompt\n",
    "        evaluation_prompt = self._create_evaluation_prompt(question_data, user_answer)\n",
    "        \n",
    "        # Call Claude API\n",
    "        try:\n",
    "            response = self._call_claude_api(evaluation_prompt)\n",
    "            return self._parse_evaluation(response)\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"API call failed: {str(e)}\"}\n",
    "    \n",
    "    def _create_evaluation_prompt(self, question_data: Dict, user_answer: str) -> str:\n",
    "        \"\"\"Create prompt for Claude evaluation.\"\"\"\n",
    "        prompt = f\"\"\"You are evaluating a student's answer to a self-supervised learning question.\n",
    "        \n",
    "Question: {question_data['question']}\n",
    "\n",
    "Evaluation Rubric (each item worth 25 points):\n",
    "{chr(10).join(f'- {item}' for item in question_data['rubric'])}\n",
    "\n",
    "Reference Answer: {question_data['sample_answer']}\n",
    "\n",
    "Student's Answer: {user_answer}\n",
    "\n",
    "Please evaluate the answer and provide a JSON response with the following structure:\n",
    "{{\n",
    "    \"score\": <number between 0-100>,\n",
    "    \"rubric_met\": [<list of rubric points that were addressed>],\n",
    "    \"strengths\": \"<what the student did well>\",\n",
    "    \"improvements\": \"<what could be improved>\",\n",
    "    \"feedback\": \"<constructive feedback for the student>\"\n",
    "}}\n",
    "\n",
    "Be encouraging but honest. Focus on understanding rather than perfect wording.\n",
    "Return ONLY the JSON object, no additional text.\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def _call_claude_api(self, prompt: str) -> str:\n",
    "        \"\"\"Call Claude API for evaluation.\"\"\"\n",
    "        if not self.client:\n",
    "            raise ValueError(\"Claude client not initialized\")\n",
    "        \n",
    "        # Use Claude Haiku for cost-effective evaluation\n",
    "        message = self.client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",  # Fast and affordable\n",
    "            max_tokens=500,\n",
    "            temperature=0.3,  # Low temperature for consistent evaluation\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return message.content[0].text\n",
    "    \n",
    "    def _parse_evaluation(self, response: str) -> Dict:\n",
    "        \"\"\"Parse Claude API response.\"\"\"\n",
    "        try:\n",
    "            # Claude might include some text before/after JSON, so we try to extract it\n",
    "            import re\n",
    "            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "            if json_match:\n",
    "                return json.loads(json_match.group())\n",
    "            else:\n",
    "                return json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"error\": \"Failed to parse API response\", \"raw_response\": response}\n",
    "    \n",
    "    def _manual_evaluation(self, question_data: Dict, user_answer: str) -> Dict:\n",
    "        \"\"\"Provide manual evaluation guidance when API is not available.\"\"\"\n",
    "        return {\n",
    "            \"message\": \"API key not configured. Please self-evaluate using the rubric below.\",\n",
    "            \"rubric\": question_data['rubric'],\n",
    "            \"sample_answer\": question_data['sample_answer'],\n",
    "            \"self_evaluation_guide\": [\n",
    "                \"Compare your answer to the sample\",\n",
    "                \"Check each rubric point (25 points each)\",\n",
    "                \"Give yourself partial credit where appropriate\",\n",
    "                \"Focus on understanding, not exact wording\"\n",
    "            ],\n",
    "            \"your_answer\": user_answer\n",
    "        }\n",
    "\n",
    "# Initialize assessment system\n",
    "assessment = OpenEndedAssessment()\n",
    "\n",
    "# Display questions\n",
    "print(\"\\nüìù Open-Ended Assessment Questions\\n\")\n",
    "print(\"=\"*50)\n",
    "for i, q in enumerate(assessment.questions, 1):\n",
    "    print(f\"\\nQuestion {i} (ID: {q['id']})\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"{q['question']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Submit and Evaluate Your Answers\n",
    "\n",
    "Use the function below to submit your answer. It will automatically evaluate it using Claude if you have an API key set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_answer(question_id: str, answer: str):\n",
    "    \"\"\"Submit and evaluate an answer.\"\"\"\n",
    "    print(f\"\\nüìä Evaluating answer for question {question_id}...\\n\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    result = assessment.evaluate_answer(question_id, answer)\n",
    "    \n",
    "    if 'error' in result:\n",
    "        print(f\"‚ùå Error: {result['error']}\")\n",
    "        if 'raw_response' in result:\n",
    "            print(f\"\\nRaw response: {result['raw_response']}\")\n",
    "    elif 'message' in result:\n",
    "        # Manual evaluation mode\n",
    "        print(f\"‚ÑπÔ∏è {result['message']}\\n\")\n",
    "        print(\"üìã Rubric (25 points each):\")\n",
    "        for i, item in enumerate(result['rubric'], 1):\n",
    "            print(f\"  {i}. {item}\")\n",
    "        print(f\"\\nüìñ Sample Answer:\\n{result['sample_answer']}\")\n",
    "        print(f\"\\n‚úçÔ∏è Your Answer:\\n{result['your_answer']}\")\n",
    "        print(\"\\nüí° Self-Evaluation Guide:\")\n",
    "        for tip in result['self_evaluation_guide']:\n",
    "            print(f\"  ‚Ä¢ {tip}\")\n",
    "    else:\n",
    "        # Automated evaluation results\n",
    "        print(f\"üéØ Score: {result['score']}/100\\n\")\n",
    "        \n",
    "        if 'rubric_met' in result and result['rubric_met']:\n",
    "            print(\"‚úÖ Rubric Points Addressed:\")\n",
    "            for point in result['rubric_met']:\n",
    "                print(f\"  ‚Ä¢ {point}\")\n",
    "        \n",
    "        if 'strengths' in result:\n",
    "            print(f\"\\nüí™ Strengths:\\n{result['strengths']}\")\n",
    "        \n",
    "        if 'improvements' in result:\n",
    "            print(f\"\\nüìà Areas for Improvement:\\n{result['improvements']}\")\n",
    "        \n",
    "        if 'feedback' in result:\n",
    "            print(f\"\\nüí¨ Feedback:\\n{result['feedback']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Submit Your Answer\n",
    "\n",
    "Try answering one of the questions and submitting it for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example answer submission (replace with your actual answer)\n",
    "my_answer_q1 = \"\"\"\n",
    "Rotation prediction is effective because it requires the network to understand \n",
    "the geometric structure of objects. When an image is rotated, the spatial \n",
    "relationships between pixels change in predictable ways. The network must learn \n",
    "features that capture these relationships to correctly predict the rotation angle. \n",
    "This is self-supervised because we can automatically generate labels by rotating \n",
    "images ourselves, requiring no human annotation.\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment to submit your answer\n",
    "# result = submit_answer(\"q1\", my_answer_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try More Questions\n",
    "\n",
    "Answer the other questions and submit them for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2: Autoencoders vs Contrastive Learning\n",
    "my_answer_q2 = \"\"\"\n",
    "Your answer here...\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment to submit\n",
    "# result = submit_answer(\"q2\", my_answer_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3: Novel Pretext Task\n",
    "my_answer_q3 = \"\"\"\n",
    "Your creative pretext task design here...\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment to submit\n",
    "# result = submit_answer(\"q3\", my_answer_q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration for Automated Assessment\n",
    "\n",
    "This configuration can be used by external systems or for tracking progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create assessment configuration for Anthropic\n",
    "assessment_config = {\n",
    "    \"lab_title\": \"Self-Supervised Neural Networks Lab\",\n",
    "    \"version\": \"2.1\",\n",
    "    \"api_configuration\": {\n",
    "        \"provider\": \"anthropic\",\n",
    "        \"model\": \"claude-3-haiku-20240307\",\n",
    "        \"temperature\": 0.3,\n",
    "        \"max_tokens\": 500,\n",
    "        \"api_key_env_var\": \"ANTHROPIC_API_KEY\"\n",
    "    },\n",
    "    \"questions\": assessment.questions,\n",
    "    \"grading_scheme\": {\n",
    "        \"open_ended\": {\n",
    "            \"q1\": {\"points\": 100, \"rubric_items\": 4},\n",
    "            \"q2\": {\"points\": 100, \"rubric_items\": 4},\n",
    "            \"q3\": {\"points\": 100, \"rubric_items\": 4}\n",
    "        },\n",
    "        \"total_points\": 300\n",
    "    },\n",
    "    \"cost_estimate\": {\n",
    "        \"model\": \"claude-3-haiku\",\n",
    "        \"input_tokens_per_eval\": \"~500\",\n",
    "        \"output_tokens_per_eval\": \"~200\",\n",
    "        \"cost_per_eval\": \"~$0.0004\",\n",
    "        \"note\": \"Haiku is 10x cheaper than Sonnet, perfect for educational use\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "with open('assessment_config_anthropic.json', 'w') as f:\n",
    "    json.dump(assessment_config, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Assessment configuration saved to assessment_config_anthropic.json\")\n",
    "print(f\"\\nüí∞ Cost estimate: ~${assessment_config['cost_estimate']['cost_per_eval']} per evaluation\")\n",
    "print(f\"   Using {assessment_config['cost_estimate']['model']} for cost-effective assessment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the System\n",
    "\n",
    "Let's test if everything is working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_assessment_system():\n",
    "    \"\"\"Test the assessment system with a sample answer.\"\"\"\n",
    "    test_answer = \"Rotation prediction works because it teaches the network about spatial relationships.\"\n",
    "    \n",
    "    print(\"üß™ Testing Assessment System\\n\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if assessment.client:\n",
    "        print(\"‚úÖ Claude API client is configured\")\n",
    "        print(\"   Submitting a test answer...\\n\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No API key found - will use manual evaluation mode\")\n",
    "        print(\"   To enable automatic evaluation:\")\n",
    "        print(\"   1. Get API key from https://console.anthropic.com/\")\n",
    "        print(\"   2. Set: export ANTHROPIC_API_KEY='your-key'\")\n",
    "        print(\"   3. Restart the notebook kernel\\n\")\n",
    "    \n",
    "    # Test with a simple answer\n",
    "    result = submit_answer(\"q1\", test_answer)\n",
    "    \n",
    "    if 'score' in result:\n",
    "        print(\"\\n‚úÖ Automatic evaluation is working!\")\n",
    "    elif 'message' in result:\n",
    "        print(\"\\n‚ÑπÔ∏è Manual evaluation mode is active\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Something went wrong\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run the test\n",
    "test_result = test_assessment_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You've Learned\n",
    "- How to set up automatic evaluation using Claude API\n",
    "- How to submit answers for assessment\n",
    "- How to use manual evaluation when API is not available\n",
    "\n",
    "### Next Steps\n",
    "1. Complete the main lab exercises in `snn_lab_interactive.ipynb`\n",
    "2. Answer the open-ended questions thoughtfully\n",
    "3. Submit your answers using this notebook for evaluation\n",
    "4. Review the feedback and improve your understanding\n",
    "\n",
    "### Cost-Effective Learning\n",
    "- We use Claude 3 Haiku for evaluation (10x cheaper than Sonnet)\n",
    "- Each evaluation costs approximately $0.0004\n",
    "- Perfect for educational environments\n",
    "\n",
    "Good luck with your self-supervised learning journey! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}