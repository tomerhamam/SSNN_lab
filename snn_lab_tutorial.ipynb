{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd0d873",
   "metadata": {},
   "source": [
    "# Self-Supervised Neural Networks Lab\n",
    "\n",
    "Welcome to the self-supervised neural networks (SNN) lab! In this tutorial you will learn how to use unlabeled data to train models that learn useful representations, and then apply them to downstream tasks. We'll explore two domains: images and time series. The lab is based on the survey we prepared and follows the curriculum described previously.\n",
    "\n",
    "Self-supervised learning (SSL) is an approach where a model learns to predict part of its input from other parts, generating supervisory signals from the data itself. This contrasts with fully supervised learning, which relies on manually annotated labels. In practice, SSL can dramatically reduce the need for labeled data and still produce high-quality representations.\n",
    "\n",
    "There are two broad families of SSL methods:\n",
    "\n",
    "- **Generative/self-predictive methods:** these methods train a model to reconstruct or predict part of the input (e.g., autoencoders, masked modeling, forecasting).\n",
    "- **Discriminative/contrastive methods:** these methods train a model to distinguish between different augmented views of the data, pulling together representations of the same instance and pushing apart representations of different instances.\n",
    "\n",
    "In this lab, we'll explore both families."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfc2609",
   "metadata": {},
   "source": [
    "## Lab Outline\n",
    "\n",
    "This lab is structured into four modules:\n",
    "\n",
    "1. **Introduction to SSL** – Concepts and categories of self-supervised learning.\n",
    "2. **Vision: rotation prediction and segmentation** – Learn a rotation prediction task and use the resulting features for digit classification.\n",
    "3. **Time-series: autoencoder and classification** – Train an autoencoder on synthetic sine sequences and classify them based on embeddings.\n",
    "4. **Evaluation and analysis** – Reflect on the results and discuss extensions.\n",
    "\n",
    "Each module includes explanatory text followed by hands-on coding exercises. Feel free to experiment with the parameters and see how the outcomes change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c92da5",
   "metadata": {},
   "source": [
    "## Module 2 – Computer Vision: Rotation Prediction and Segmentation\n",
    "\n",
    "We'll use the digits dataset from scikit-learn, which contains 1797 images of handwritten digits. Each image is 8×8 pixels, resulting in 64 features per sample. Pixel values are integers in the range 0 to 16. We'll begin by loading the dataset and visualizing some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0014f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the digits dataset and split into features X and labels y\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Visualize the first 10 images\n",
    "fig, axes = plt.subplots(1, 10, figsize=(10, 1))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(digits.images[i], cmap='gray')\n",
    "    ax.set_title(str(digits.target[i]))\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X = X.astype('float32') / 16.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c611992",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Create a dataset of rotated images for the pretext task\n",
    "def create_rotation_dataset(X):\n",
    "    images = X.reshape(-1, 8, 8)\n",
    "    rotations = [0, 90, 180, 270]\n",
    "    rot_images = []\n",
    "    rot_labels = []\n",
    "    for idx, angle in enumerate(rotations):\n",
    "        k = (angle // 90) % 4\n",
    "        for img in images:\n",
    "            rotated = np.rot90(img, k=k)\n",
    "            rot_images.append(rotated.flatten())\n",
    "            rot_labels.append(idx)\n",
    "    return np.array(rot_images, dtype=np.float32), np.array(rot_labels, dtype=np.int64)\n",
    "\n",
    "rot_X, rot_y = create_rotation_dataset(X)\n",
    "print(\"Rotation dataset size:\", rot_X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5014122",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TwoLayerNet:\n",
    "    # A simple two-layer neural network trained via gradient descent.\n",
    "    input_dim: int\n",
    "    hidden_dim: int\n",
    "    output_dim: int\n",
    "    learning_rate: float = 0.5\n",
    "\n",
    "    def __post_init__(self):\n",
    "        rng = np.random.default_rng(0)\n",
    "        self.W1 = rng.standard_normal((self.input_dim, self.hidden_dim)) * 0.01\n",
    "        self.b1 = np.zeros(self.hidden_dim)\n",
    "        self.W2 = rng.standard_normal((self.hidden_dim, self.output_dim)) * 0.01\n",
    "        self.b2 = np.zeros(self.output_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        z1 = X.dot(self.W1) + self.b1\n",
    "        a1 = np.tanh(z1)\n",
    "        z2 = a1.dot(self.W2) + self.b2\n",
    "        exp_scores = np.exp(z2 - np.max(z2, axis=1, keepdims=True))\n",
    "        probs = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n",
    "        cache = (X, z1, a1, z2, probs)\n",
    "        return probs, cache\n",
    "\n",
    "    def backward(self, cache, y_true):\n",
    "        X, z1, a1, z2, probs = cache\n",
    "        n_samples = X.shape[0]\n",
    "        one_hot = np.zeros_like(probs)\n",
    "        one_hot[np.arange(n_samples), y_true] = 1\n",
    "        dz2 = (probs - one_hot) / n_samples\n",
    "        dW2 = a1.T.dot(dz2)\n",
    "        db2 = dz2.sum(axis=0)\n",
    "        da1 = dz2.dot(self.W2.T)\n",
    "        dz1 = da1 * (1.0 - np.tanh(z1)**2)\n",
    "        dW1 = X.T.dot(dz1)\n",
    "        db1 = dz1.sum(axis=0)\n",
    "        return dW1, db1, dW2, db2\n",
    "\n",
    "    def update_params(self, dW1, db1, dW2, db2):\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "\n",
    "    def train(self, X, y, epochs=20, batch_size=128):\n",
    "        n_samples = X.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            idx = np.random.permutation(n_samples)\n",
    "            X_shuf, y_shuf = X[idx], y[idx]\n",
    "            for start in range(0, n_samples, batch_size):\n",
    "                end = start + batch_size\n",
    "                X_batch = X_shuf[start:end]\n",
    "                y_batch = y_shuf[start:end]\n",
    "                probs, cache = self.forward(X_batch)\n",
    "                grads = self.backward(cache, y_batch)\n",
    "                self.update_params(*grads)\n",
    "\n",
    "    def hidden_representation(self, X):\n",
    "        z1 = X.dot(self.W1) + self.b1\n",
    "        return np.tanh(z1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs, _ = self.forward(X)\n",
    "        return probs.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b06ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Instantiate the network\n",
    "net = TwoLayerNet(input_dim=64, hidden_dim=32, output_dim=4, learning_rate=0.3)\n",
    "\n",
    "# Split rotation dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(rot_X, rot_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the network on the rotation task\n",
    "net.train(X_train, y_train, epochs=15, batch_size=256)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "val_preds = net.predict(X_val)\n",
    "val_acc = (val_preds == y_val).mean()\n",
    "print(f\"Rotation classification accuracy: {val_acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca9f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract hidden representations for all digits\n",
    "features = net.hidden_representation(X)\n",
    "\n",
    "# Train/test split for downstream digit classification\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(features, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Train a logistic regression classifier on the SSL features\n",
    "clf_feat = LogisticRegression(max_iter=200, multi_class='auto', solver='lbfgs')\n",
    "clf_feat.fit(X_train_d, y_train_d)\n",
    "feat_acc = clf_feat.score(X_test_d, y_test_d)\n",
    "print(f\"Digit classification accuracy using SSL features: {feat_acc:.2f}\")\n",
    "\n",
    "# Baseline classifier on raw pixel values\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "clf_base = LogisticRegression(max_iter=200, multi_class='auto', solver='lbfgs')\n",
    "clf_base.fit(X_train_raw, y_train_raw)\n",
    "base_acc = clf_base.score(X_test_raw, y_test_raw)\n",
    "print(f\"Baseline classification accuracy on raw pixels: {base_acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2a549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Perform simple KMeans segmentation on the first digit image\n",
    "sample_img = digits.images[0]\n",
    "pixels = sample_img.reshape(-1, 1)\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(pixels)\n",
    "segmented = kmeans.labels_.reshape(sample_img.shape)\n",
    "\n",
    "# Visualize the segmentation result\n",
    "plt.imshow(segmented, cmap='gray')\n",
    "plt.title(\"Simple KMeans segmentation of a digit\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28021100",
   "metadata": {},
   "source": [
    "## Module 3 – Time Series: Autoencoder and Classification\n",
    "\n",
    "In this module we switch to the time-series domain. We'll generate synthetic sine-wave sequences with two different frequencies and add Gaussian noise. The self-supervised task will be to reconstruct the sequences using an autoencoder (a generative self-supervised method). We will then classify the sequences based on the autoencoder's hidden representations and compare the performance to a classifier trained on raw sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b434c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic sine sequences\n",
    "def generate_sine_sequences(n_samples=1000, length=50, freq0=1.0, freq1=3.0, noise_std=0.1):\n",
    "    t = np.linspace(0, 2 * np.pi, length)\n",
    "    half = n_samples // 2\n",
    "    seq0 = np.sin(freq0 * t)[None, :] * np.ones((half, 1))\n",
    "    seq1 = np.sin(freq1 * t)[None, :] * np.ones((n_samples - half, 1))\n",
    "    X = np.concatenate([seq0, seq1], axis=0)\n",
    "    X += np.random.normal(scale=noise_std, size=X.shape)\n",
    "    y = np.concatenate([np.zeros(half, dtype=int), np.ones(n_samples - half, dtype=int)])\n",
    "    return X.astype(np.float32), y\n",
    "\n",
    "X_ts, y_ts = generate_sine_sequences()\n",
    "\n",
    "# Visualize one example from each class\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(X_ts[0], label=\"Class 0\")\n",
    "plt.plot(X_ts[-1], label=\"Class 1\")\n",
    "plt.title(\"Example sine sequences\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90dd0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Autoencoder:\n",
    "    # A simple one-hidden-layer autoencoder for sequences.\n",
    "    input_dim: int\n",
    "    hidden_dim: int\n",
    "    learning_rate: float = 0.1\n",
    "\n",
    "    def __post_init__(self):\n",
    "        rng = np.random.default_rng(1)\n",
    "        self.W_enc = rng.standard_normal((self.input_dim, self.hidden_dim)) * 0.05\n",
    "        self.b_enc = np.zeros(self.hidden_dim)\n",
    "        self.W_dec = rng.standard_normal((self.hidden_dim, self.input_dim)) * 0.05\n",
    "        self.b_dec = np.zeros(self.input_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        z = X.dot(self.W_enc) + self.b_enc\n",
    "        h = np.tanh(z)\n",
    "        recon = h.dot(self.W_dec) + self.b_dec\n",
    "        return h, recon\n",
    "\n",
    "    def backward(self, X, h, recon):\n",
    "        n_samples = X.shape[0]\n",
    "        d_recon = (recon - X) / n_samples\n",
    "        dW_dec = h.T.dot(d_recon)\n",
    "        db_dec = d_recon.sum(axis=0)\n",
    "        dh = d_recon.dot(self.W_dec.T)\n",
    "        dz = dh * (1.0 - h**2)\n",
    "        dW_enc = X.T.dot(dz)\n",
    "        db_enc = dz.sum(axis=0)\n",
    "        return dW_enc, db_enc, dW_dec, db_dec\n",
    "\n",
    "    def update_params(self, dW_enc, db_enc, dW_dec, db_dec):\n",
    "        self.W_enc -= self.learning_rate * dW_enc\n",
    "        self.b_enc -= self.learning_rate * db_enc\n",
    "        self.W_dec -= self.learning_rate * dW_dec\n",
    "        self.b_dec -= self.learning_rate * db_dec\n",
    "\n",
    "    def train(self, X, epochs=30, batch_size=64):\n",
    "        n_samples = X.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            idx = np.random.permutation(n_samples)\n",
    "            X_shuf = X[idx]\n",
    "            for start in range(0, n_samples, batch_size):\n",
    "                end = start + batch_size\n",
    "                X_batch = X_shuf[start:end]\n",
    "                h, recon = self.forward(X_batch)\n",
    "                grads = self.backward(X_batch, h, recon)\n",
    "                self.update_params(*grads)\n",
    "\n",
    "    def encode(self, X):\n",
    "        z = X.dot(self.W_enc) + self.b_enc\n",
    "        return np.tanh(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba00114",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split the time series dataset\n",
    "X_train_ts, X_test_ts, y_train_ts, y_test_ts = train_test_split(X_ts, y_ts, test_size=0.3, random_state=0)\n",
    "\n",
    "# Normalize sequences to zero mean and unit variance\n",
    "X_train_norm = (X_train_ts - X_train_ts.mean(axis=1, keepdims=True)) / (X_train_ts.std(axis=1, keepdims=True) + 1e-6)\n",
    "X_test_norm = (X_test_ts - X_test_ts.mean(axis=1, keepdims=True)) / (X_test_ts.std(axis=1, keepdims=True) + 1e-6)\n",
    "\n",
    "# Train the autoencoder\n",
    "ae = Autoencoder(input_dim=X_train_norm.shape[1], hidden_dim=16, learning_rate=0.05)\n",
    "ae.train(X_train_norm, epochs=30, batch_size=128)\n",
    "\n",
    "# Encode sequences\n",
    "train_emb = ae.encode(X_train_norm)\n",
    "test_emb = ae.encode(X_test_norm)\n",
    "\n",
    "# Classifier on autoencoder embeddings\n",
    "clf_emb = LogisticRegression(max_iter=200, multi_class='auto', solver='lbfgs')\n",
    "clf_emb.fit(train_emb, y_train_ts)\n",
    "emb_acc = clf_emb.score(test_emb, y_test_ts)\n",
    "print(f\"Time-series classification accuracy using SSL features: {emb_acc:.2f}\")\n",
    "\n",
    "# Baseline classifier on raw normalized sequences\n",
    "clf_raw = LogisticRegression(max_iter=200, multi_class='auto', solver='lbfgs')\n",
    "clf_raw.fit(X_train_norm, y_train_ts)\n",
    "raw_acc = clf_raw.score(X_test_norm, y_test_ts)\n",
    "print(f\"Baseline classification accuracy on raw sequences: {raw_acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ba200a",
   "metadata": {},
   "source": [
    "## Evaluation and Analysis\n",
    "\n",
    "In the experiments above, you should observe that the rotation classifier performs significantly better than random guessing, indicating that the model has learned to understand orientation. However, the transfer of this representation to digit classification may not surpass a classifier trained directly on raw pixels (since the digits dataset is small and relatively easy to classify). The important takeaway is that self-supervised pretraining can produce non-trivial features even without labels.\n",
    "\n",
    "For the time-series example, the autoencoder compresses each sequence into a lower-dimensional hidden vector. The classifier trained on these embeddings should perform comparably to the classifier trained on raw sequences, showing that the autoencoder has learned a useful representation that retains most of the information.\n",
    "\n",
    "Here are some questions to consider:\n",
    "\n",
    "- How would performance change if you increased the hidden dimension or the number of epochs?\n",
    "- Can you think of other pretext tasks for images or time series (e.g., masking patches, permuting segments, predicting transformations)?\n",
    "- How might contrastive methods (e.g., SimCLR or MoCo) improve the learned features compared to the simple autoencoder?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
