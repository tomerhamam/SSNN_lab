{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Supervised Neural Networks: Complete Interactive Lab\n",
    "\n",
    "Welcome to an immersive journey through self-supervised learning! This lab combines theory, hands-on coding, and AI-powered assessment to give you a deep understanding of SSL principles.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "1. **Understand SSL fundamentals**: Distinguish between generative and discriminative approaches\n",
    "2. **Implement pretext tasks**: Create rotation prediction and autoencoder tasks from scratch\n",
    "3. **Build neural networks**: Implement forward/backward propagation using only NumPy\n",
    "4. **Apply transfer learning**: Use SSL features for downstream classification tasks\n",
    "5. **Analyze representations**: Visualize and interpret learned features\n",
    "6. **Design SSL systems**: Create novel pretext tasks for different domains\n",
    "\n",
    "## üìö Prerequisites\n",
    "- **Math**: Linear algebra, basic calculus (gradients)\n",
    "- **Programming**: Python, NumPy fundamentals\n",
    "- **ML Basics**: Neural networks, supervised learning concepts\n",
    "\n",
    "## ‚è±Ô∏è Expected Duration: ~2 hours\n",
    "- Part 1: Introduction & Setup (15 min)\n",
    "- Part 2: Computer Vision SSL (45 min)  \n",
    "- Part 3: Time Series SSL (30 min)\n",
    "- Part 4: Advanced Concepts (15 min)\n",
    "- Part 5: Assessment & Reflection (15 min)\n",
    "\n",
    "## üîó Background Reading (Optional)\n",
    "- [Self-supervised Learning Survey](https://arxiv.org/abs/2301.05712)\n",
    "- [Representation Learning Review](https://arxiv.org/abs/1206.5538)\n",
    "\n",
    "Let's begin! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Introduction & Setup (15 minutes)\n",
    "\n",
    "## What is Self-Supervised Learning?\n",
    "\n",
    "**Self-supervised learning (SSL)** is a learning paradigm where models create their own supervision signals from the data structure itself, without requiring human-annotated labels.\n",
    "\n",
    "### The Key Insight\n",
    "Instead of learning from `(input, human_label)` pairs, SSL learns from `(input, automatically_generated_label)` pairs by solving **pretext tasks**.\n",
    "\n",
    "### Two Main Families\n",
    "\n",
    "#### 1. üé® **Generative/Predictive Methods**\n",
    "- **Goal**: Reconstruct or predict part of the input\n",
    "- **Examples**: Autoencoders, masked language modeling (BERT), image inpainting\n",
    "- **Learning signal**: Reconstruction error\n",
    "\n",
    "#### 2. üîÑ **Discriminative/Contrastive Methods**  \n",
    "- **Goal**: Learn to distinguish between different views of data\n",
    "- **Examples**: SimCLR, MoCo, rotation prediction\n",
    "- **Learning signal**: Similarity/dissimilarity between samples\n",
    "\n",
    "### Why SSL Matters\n",
    "- **Abundant unlabeled data**: Most real-world data lacks labels\n",
    "- **Expensive annotation**: Expert labeling costs time and money\n",
    "- **Better representations**: SSL can learn general features useful across tasks\n",
    "- **Data efficiency**: Reduces labeled data requirements for downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Environment setup complete!\n",
      "üìä NumPy version: 2.3.2\n",
      "üöÄ Ready to start learning SSL!\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"üîß Environment setup complete!\")\n",
    "print(f\"üìä NumPy version: {np.__version__}\")\n",
    "print(\"üöÄ Ready to start learning SSL!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Exercise 1: Data Exploration (5 minutes)\n",
    "\n",
    "Let's start by exploring our datasets. You'll implement a function to visualize data samples.\n",
    "\n",
    "**Your Task**: Complete the `visualize_samples` function to display a grid of samples with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset loaded: 1797 samples, 64 features\n",
      "üî¢ Classes: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_samples(images, labels=None, title=\"Sample Visualization\", n_samples=10, figsize=(12, 2)):\n",
    "    \"\"\"\n",
    "    Visualize a grid of image samples.\n",
    "    \n",
    "    TODO: Complete this function\n",
    "    Args:\n",
    "        images: Array of shape (n_samples, height, width) or (n_samples, height*width)\n",
    "        labels: Optional labels for each image\n",
    "        title: Plot title\n",
    "        n_samples: Number of samples to show\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    # Ensure we don't try to show more samples than available\n",
    "    n_show = min(n_samples, len(images))\n",
    "    \n",
    "    # TODO: Create figure and subplots\n",
    "    fig, axes = plt.subplots(1, n_show, figsize=figsize)\n",
    "    \n",
    "    # Handle single subplot case\n",
    "    if n_show == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i in range(n_show):\n",
    "        # TODO: Get the image and reshape if needed\n",
    "        img = images[i]\n",
    "        if len(img.shape) == 1:  # If flattened\n",
    "            img = img.reshape(8, 8) # FILL: Reshape to (8, 8) for digits\n",
    "\n",
    "        # TODO: Display the image\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        \n",
    "        # TODO: Set title with label if provided\n",
    "        if labels is not None:\n",
    "            axes[i].set_title(f\"Label: {labels[i]}\")\n",
    "\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test your implementation\n",
    "digits = load_digits()\n",
    "print(f\"üìä Dataset loaded: {digits.data.shape[0]} samples, {digits.data.shape[1]} features\")\n",
    "print(f\"üî¢ Classes: {np.unique(digits.target)}\")\n",
    "\n",
    "# TODO: Uncomment to test your visualization function\n",
    "visualize_samples(digits.images, digits.target, \"Handwritten Digits Dataset\", n_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution Check\n",
    "If your implementation is correct, you should see a row of 10 digit images with their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (run this cell if you need help)\n",
    "def visualize_samples_solution(images, labels=None, title=\"Sample Visualization\", n_samples=10, figsize=(12, 2)):\n",
    "    \"\"\"Reference implementation for visualization function.\"\"\"\n",
    "    n_show = min(n_samples, len(images))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_show, figsize=figsize)\n",
    "    \n",
    "    if n_show == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i in range(n_show):\n",
    "        img = images[i]\n",
    "        if len(img.shape) == 1:\n",
    "            img = img.reshape(8, 8)\n",
    "        \n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        \n",
    "        if labels is not None:\n",
    "            axes[i].set_title(f\"Label: {labels[i]}\")\n",
    "        \n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test the solution\n",
    "visualize_samples_solution(digits.images, digits.target, \"Handwritten Digits - Solution\", n_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Critical Thinking Question 1\n",
    "\n",
    "**Looking at these 8√ó8 pixel images, why do you think such low resolution is sufficient for humans to recognize digits? What does this tell us about the kind of features a neural network needs to learn?**\n",
    "\n",
    "*Think about: shape, edges, structure, and what makes each digit distinctive.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Computer Vision SSL - Rotation Prediction (45 minutes)\n",
    "\n",
    "Now we'll implement our first SSL system using **rotation prediction** as a pretext task.\n",
    "\n",
    "## The Big Picture\n",
    "1. **Pretext Task**: Train a network to predict image rotations (0¬∞, 90¬∞, 180¬∞, 270¬∞)\n",
    "2. **Feature Learning**: Network learns spatial features to solve rotation task\n",
    "3. **Transfer Learning**: Use learned features for digit classification\n",
    "4. **Evaluation**: Compare SSL features vs raw pixels\n",
    "\n",
    "## Why Rotation Prediction Works\n",
    "- **Geometric Understanding**: Requires understanding object structure and orientation\n",
    "- **Rich Features**: Forces network to learn rotation-equivariant representations  \n",
    "- **Free Labels**: Can generate unlimited rotation labels automatically\n",
    "- **General Purpose**: Features useful for many downstream vision tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Exercise 2: Create Rotation Dataset (10 minutes)\n",
    "\n",
    "First, let's create our pretext task dataset by generating rotated versions of images.\n",
    "\n",
    "**Your Task**: Complete the rotation dataset creation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_rotation_dataset(X: np.ndarray, rotations: Tuple[int, ...] = (0, 90, 180, 270)) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Create a dataset of rotated images for the pretext task.\n    \n    This function implements the core of rotation prediction SSL:\n    1. Takes original images and creates rotated versions\n    2. Generates labels based on rotation angle (0=0¬∞, 1=90¬∞, 2=180¬∞, 3=270¬∞)\n    3. Returns expanded dataset with 4x more samples\n    \n    Args:\n        X: Array of flattened images, shape (n_samples, 64)\n           Each row is a flattened 8x8 digit image\n        rotations: Tuple of rotation angles in degrees (default: 0, 90, 180, 270)\n    \n    Returns:\n        rot_X: Array of rotated images, shape (n_samples * len(rotations), 64)\n               Each original image becomes len(rotations) samples\n        rot_y: Array of rotation labels, shape (n_samples * len(rotations),)\n               0=0¬∞, 1=90¬∞, 2=180¬∞, 3=270¬∞\n    \"\"\"\n    # TODO: Reshape flattened images back to 8x8\n    # The sklearn digits dataset gives us flattened 64D vectors\n    # We need 2D images to apply rotations\n    images = X.reshape(-1, 8, 8) # FILL: (-1, 8, 8)\n    \n    # Lists to collect all rotated images and their labels\n    rot_images = []\n    rot_labels = []\n    \n    # Create rotated versions for each angle\n    for idx, angle in enumerate(rotations):\n        # TODO: Calculate number of 90-degree rotations needed\n        # np.rot90 rotates by 90¬∞ each time, so we need k rotations for angle¬∞\n        # Examples: 0¬∞ ‚Üí k=0, 90¬∞ ‚Üí k=1, 180¬∞ ‚Üí k=2, 270¬∞ ‚Üí k=3\n        # Hint: np.rot90 rotates by 90 degrees k times\n        k = (angle // 90) % 4  # FILL: (angle // 90) % 4\n\n        # Apply this rotation to every image\n        for img in images:\n            # TODO: Rotate the image k times by 90 degrees\n            # np.rot90(image, k) rotates image by k*90 degrees counterclockwise\n            rotated = np.rot90(img, k=k)\n\n            # TODO: Flatten and add to lists\n            # We need to flatten back to 1D for neural network input\n            # Each rotation gets a label corresponding to its angle class\n            rot_images.append(rotated.flatten())  # Convert 8x8 ‚Üí 64D vector\n            rot_labels.append(idx)  # idx maps to rotation class\n\n    # Convert lists to NumPy arrays with proper dtypes\n    # float32 for images (neural networks prefer 32-bit), int64 for labels\n    return np.array(rot_images, dtype=np.float32), np.array(rot_labels, dtype=np.int64)\n\n# Load and normalize digit data\ndigits = load_digits()\nX = digits.data.astype(np.float32) / 16.0  # Normalize to [0, 1]\ny = digits.target\n\nprint(f\"üìä Original dataset: {X.shape}\")\nprint(f\"üéØ Original classes: {len(np.unique(y))} digits (0-9)\")\n\n# TODO: Create rotation dataset using first 100 samples for testing\nrot_X, rot_y = create_rotation_dataset(X[:100])\nprint(f\"üîÑ Rotation dataset: {rot_X.shape}\")\nprint(f\"üè∑Ô∏è Rotation classes: {np.unique(rot_y)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize the rotations to check our implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution and visualization\n",
    "def create_rotation_dataset_solution(X: np.ndarray, rotations: Tuple[int, ...] = (0, 90, 180, 270)) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    images = X.reshape(-1, 8, 8)\n",
    "    rot_images = []\n",
    "    rot_labels = []\n",
    "    \n",
    "    for idx, angle in enumerate(rotations):\n",
    "        k = (angle // 90) % 4\n",
    "        for img in images:\n",
    "            rotated = np.rot90(img, k=k)\n",
    "            rot_images.append(rotated.flatten())\n",
    "            rot_labels.append(idx)\n",
    "    \n",
    "    return np.array(rot_images, dtype=np.float32), np.array(rot_labels, dtype=np.int64)\n",
    "\n",
    "# Create rotation dataset\n",
    "rot_X, rot_y = create_rotation_dataset_solution(X[:100])\n",
    "print(f\"üîÑ Rotation dataset shape: {rot_X.shape}\")\n",
    "print(f\"üè∑Ô∏è Rotation labels: {np.unique(rot_y)} (0=0¬∞, 1=90¬∞, 2=180¬∞, 3=270¬∞)\")\n",
    "\n",
    "# Visualize rotations of a single digit\n",
    "sample_idx = 0  # First digit\n",
    "rotations = [0, 90, 180, 270]\n",
    "sample_rotations = []\n",
    "\n",
    "for i in range(4):\n",
    "    # Each rotation class contains the same digit rotated differently\n",
    "    rot_sample_idx = sample_idx + i * 100  # 100 original samples per rotation\n",
    "    sample_rotations.append(rot_X[rot_sample_idx])\n",
    "\n",
    "visualize_samples_solution(\n",
    "    sample_rotations, \n",
    "    [f\"{angle}¬∞\" for angle in rotations],\n",
    "    f\"Rotations of Digit {y[sample_idx]}\", \n",
    "    n_samples=4,\n",
    "    figsize=(8, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Exercise 3: Neural Network Implementation (15 minutes)\n",
    "\n",
    "Now we'll implement a simple 2-layer neural network from scratch to solve the rotation prediction task.\n",
    "\n",
    "**Your Task**: Complete the forward pass and key methods of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "    def __post_init__(self):\n        \"\"\"Initialize network parameters using Xavier-like initialization.\"\"\"\n        # Use fixed seed for reproducible results across runs\n        rng = np.random.default_rng(0)\n        \n        # TODO: Initialize weights and biases\n        # Weight initialization is crucial for neural network training\n        # - Too large: gradients explode, training unstable\n        # - Too small: gradients vanish, learning too slow\n        # - We use small random weights (multiply by 0.01) for stability\n        # - Biases typically start at zero\n        \n        # Layer 1: Input ‚Üí Hidden\n        # Shape: (input_dim, hidden_dim) so that input @ W1 gives (batch_size, hidden_dim)\n        self.W1 = rng.standard_normal((self.input_dim, self.hidden_dim)) * 0.01 # FILL: (self.input_dim, self.hidden_dim)\n        self.b1 = np.zeros(self.hidden_dim) # FILL: self.hidden_dim\n        \n        # Layer 2: Hidden ‚Üí Output  \n        # Shape: (hidden_dim, output_dim) so that hidden @ W2 gives (batch_size, output_dim)\n        self.W2 = rng.standard_normal((self.hidden_dim, self.output_dim)) * 0.01 # FILL: (self.hidden_dim, self.output_dim) \n        self.b2 = np.zeros(self.output_dim) # FILL: self.output_dim"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Quick Test: Forward Pass\n",
    "Let's test if your forward pass implementation works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass with small batch\n",
    "test_input = rot_X[:5]  # 5 samples\n",
    "probs, cache = test_net.forward(test_input)\n",
    "\n",
    "print(f\"‚úÖ Forward pass successful!\")\n",
    "print(f\"üìä Input shape: {test_input.shape}\")\n",
    "print(f\"üìä Output probabilities shape: {probs.shape}\")\n",
    "print(f\"üéØ Probability sums (should be ~1.0): {probs.sum(axis=1)}\")\n",
    "print(f\"üîç Sample predictions: {probs.argmax(axis=1)}\")\n",
    "\n",
    "# Verify probabilities sum to 1\n",
    "assert np.allclose(probs.sum(axis=1), 1.0), \"Probabilities should sum to 1!\"\n",
    "print(\"‚úÖ Forward pass test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Exercise 4: Train the Rotation Classifier (10 minutes)\n",
    "\n",
    "Now let's train our network on the rotation prediction task!\n",
    "\n",
    "**Your Task**: Set up training and evaluate the rotation classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full rotation dataset\n",
    "print(\"üîÑ Creating full rotation dataset...\")\n",
    "rot_X, rot_y = create_rotation_dataset_solution(X)  # Use all samples\n",
    "print(f\"üìä Full rotation dataset: {rot_X.shape}\")\n",
    "\n",
    "# TODO: Split into training and validation sets\n",
    "# Hint: Use 80/20 split with random_state=42 for reproducibility\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    ___,  # FILL: rot_X\n",
    "    ___,  # FILL: rot_y\n",
    "    test_size=___,  # FILL: 0.2\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üìä Training set: {X_train.shape}\")\n",
    "print(f\"üìä Validation set: {X_val.shape}\")\n",
    "\n",
    "# TODO: Create and configure the network\n",
    "# Hint: Use input_dim=64, hidden_dim=32, output_dim=4\n",
    "net = TwoLayerNet(\n",
    "    input_dim=___,  # FILL: 64\n",
    "    hidden_dim=___,  # FILL: 32\n",
    "    output_dim=___,  # FILL: 4\n",
    "    learning_rate=0.3\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Starting training...\")\n",
    "\n",
    "# TODO: Train the network\n",
    "# Hint: Use 15 epochs, batch_size=256\n",
    "# losses, accuracies = net.train(X_train, y_train, epochs=___, batch_size=___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Train the network\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    rot_X, rot_y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "net = TwoLayerNet(input_dim=64, hidden_dim=32, output_dim=4, learning_rate=0.3)\n",
    "\n",
    "print(\"üöÄ Training rotation classifier...\")\n",
    "losses, accuracies = net.train(X_train, y_train, epochs=15, batch_size=256)\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_acc = net.evaluate(X_val, y_val)\n",
    "print(f\"\\nüéØ Final validation accuracy: {val_acc:.3f}\")\n",
    "\n",
    "# Check if we beat random guessing (25% for 4 classes)\n",
    "if val_acc > 0.25:\n",
    "    print(f\"‚úÖ Great! We beat random guessing (25%)\")\n",
    "    if val_acc > 0.5:\n",
    "        print(f\"üéâ Excellent! The network learned meaningful rotation features!\")\n",
    "else:\n",
    "    print(f\"‚ùå Hmm, we didn't beat random guessing. Try adjusting hyperparameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curve\n",
    "ax1.plot(losses, 'b-', linewidth=2, label='Training Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Cross-Entropy Loss')\n",
    "ax1.set_title('Training Loss Over Time')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Accuracy curve\n",
    "ax2.plot(accuracies, 'g-', linewidth=2, label='Training Accuracy')\n",
    "ax2.axhline(y=0.25, color='r', linestyle='--', alpha=0.7, label='Random Guess (25%)')\n",
    "ax2.axhline(y=val_acc, color='orange', linestyle='--', alpha=0.7, label=f'Final Val Acc ({val_acc:.3f})')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training Accuracy Over Time')\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Training Summary:\")\n",
    "print(f\"   ‚Ä¢ Final training accuracy: {accuracies[-1]:.3f}\")\n",
    "print(f\"   ‚Ä¢ Final validation accuracy: {val_acc:.3f}\")\n",
    "print(f\"   ‚Ä¢ Improvement over random: {(val_acc - 0.25) * 100:.1f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Exercise 5: Transfer Learning Analysis (10 minutes)\n",
    "\n",
    "The real test of SSL: Can we use the learned features for a different task?\n",
    "\n",
    "**Your Task**: Extract features and compare SSL vs baseline performance on digit classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Setting up transfer learning experiment...\")\n",
    "\n",
    "# TODO: Extract hidden features for all original digits\n",
    "ssl_features = ___  # FILL: net.hidden_representation(X)\n",
    "\n",
    "print(f\"üìä Original digit data: {X.shape}\")\n",
    "print(f\"üß† SSL features: {ssl_features.shape}\")\n",
    "print(f\"üìâ Dimensionality reduction: {X.shape[1]} ‚Üí {ssl_features.shape[1]} features\")\n",
    "\n",
    "# TODO: Split data for downstream classification\n",
    "# Create train/test splits for both SSL features and raw pixels\n",
    "X_train_ssl, X_test_ssl, y_train_ssl, y_test_ssl = train_test_split(\n",
    "    ___,  # FILL: ssl_features\n",
    "    ___,  # FILL: y  \n",
    "    test_size=0.3,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(\n",
    "    ___,  # FILL: X\n",
    "    ___,  # FILL: y\n",
    "    test_size=0.3, \n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Downstream task splits:\")\n",
    "print(f\"   ‚Ä¢ SSL features train/test: {X_train_ssl.shape} / {X_test_ssl.shape}\")\n",
    "print(f\"   ‚Ä¢ Raw pixels train/test: {X_train_raw.shape} / {X_test_raw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution and training\n",
    "ssl_features = net.hidden_representation(X)\n",
    "\n",
    "X_train_ssl, X_test_ssl, y_train_ssl, y_test_ssl = train_test_split(\n",
    "    ssl_features, y, test_size=0.3, random_state=1\n",
    ")\n",
    "\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=1\n",
    ")\n",
    "\n",
    "print(\"üöÄ Training downstream classifiers...\")\n",
    "\n",
    "# Train classifier on SSL features\n",
    "clf_ssl = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf_ssl.fit(X_train_ssl, y_train_ssl)\n",
    "ssl_acc = clf_ssl.score(X_test_ssl, y_test_ssl)\n",
    "\n",
    "# Train baseline classifier on raw pixels\n",
    "clf_baseline = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf_baseline.fit(X_train_raw, y_train_raw)\n",
    "baseline_acc = clf_baseline.score(X_test_raw, y_test_raw)\n",
    "\n",
    "print(f\"\\nüìä Transfer Learning Results:\")\n",
    "print(f\"   üß† SSL Features Accuracy: {ssl_acc:.3f}\")\n",
    "print(f\"   üì∏ Raw Pixels Accuracy: {baseline_acc:.3f}\")\n",
    "print(f\"   üìà SSL vs Baseline: {(ssl_acc - baseline_acc)*100:+.1f} percentage points\")\n",
    "\n",
    "if ssl_acc > baseline_acc:\n",
    "    print(f\"   ‚úÖ SSL features outperform raw pixels!\")\n",
    "elif abs(ssl_acc - baseline_acc) < 0.02:\n",
    "    print(f\"   üìä SSL features perform similarly to raw pixels\")\n",
    "else:\n",
    "    print(f\"   üìù Raw pixels perform better (dataset might be too simple for SSL to shine)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Detailed Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed performance comparison\n",
    "ssl_pred = clf_ssl.predict(X_test_ssl)\n",
    "baseline_pred = clf_baseline.predict(X_test_raw)\n",
    "\n",
    "# Confusion matrices\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# SSL features confusion matrix\n",
    "cm_ssl = confusion_matrix(y_test_ssl, ssl_pred)\n",
    "sns.heatmap(cm_ssl, annot=True, fmt='d', cmap='Blues', ax=ax1, cbar=False)\n",
    "ax1.set_title(f'SSL Features (Acc: {ssl_acc:.3f})')\n",
    "ax1.set_xlabel('Predicted')\n",
    "ax1.set_ylabel('True')\n",
    "\n",
    "# Baseline confusion matrix  \n",
    "cm_baseline = confusion_matrix(y_test_raw, baseline_pred)\n",
    "sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Oranges', ax=ax2, cbar=False)\n",
    "ax2.set_title(f'Raw Pixels (Acc: {baseline_acc:.3f})')\n",
    "ax2.set_xlabel('Predicted')\n",
    "ax2.set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class performance\n",
    "print(\"\\nüìä Per-class Performance Comparison:\")\n",
    "print(\"\\nSSL Features:\")\n",
    "print(classification_report(y_test_ssl, ssl_pred, target_names=[str(i) for i in range(10)]))\n",
    "\n",
    "print(\"\\nRaw Pixels:\")\n",
    "print(classification_report(y_test_raw, baseline_pred, target_names=[str(i) for i in range(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Critical Thinking Question 2\n",
    "\n",
    "**Looking at the results, why might SSL features sometimes underperform raw pixels on this simple dataset? In what scenarios would you expect SSL to provide bigger advantages?**\n",
    "\n",
    "*Think about: dataset complexity, label availability, domain shift, and the information bottleneck.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Multiple Choice Question 1\n",
    "**What is the main advantage of using rotation prediction as a pretext task?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleChoiceQuestion:\n",
    "    def __init__(self, question, options, correct_answer, explanation):\n",
    "        self.question = question\n",
    "        self.options = options\n",
    "        self.correct_answer = correct_answer\n",
    "        self.explanation = explanation\n",
    "    \n",
    "    def display(self):\n",
    "        print(f\"‚ùì {self.question}\\n\")\n",
    "        for i, option in enumerate(self.options, 1):\n",
    "            print(f\"{i}. {option}\")\n",
    "        print(\"\\nEnter your answer (1-4):\")\n",
    "    \n",
    "    def check_answer(self, answer):\n",
    "        if answer == self.correct_answer:\n",
    "            print(\"‚úÖ Correct!\")\n",
    "        else:\n",
    "            print(f\"‚ùå Incorrect. The correct answer is {self.correct_answer}.\")\n",
    "        print(f\"\\nüí° Explanation: {self.explanation}\")\n",
    "\n",
    "mcq1 = MultipleChoiceQuestion(\n",
    "    \"What is the main advantage of using rotation prediction as a pretext task?\",\n",
    "    [\n",
    "        \"It's computationally very fast to train\",\n",
    "        \"It forces the network to learn geometric and spatial features\",\n",
    "        \"It works only on handwritten digits\",\n",
    "        \"It requires very little data to be effective\"\n",
    "    ],\n",
    "    2,\n",
    "    \"Rotation prediction forces the network to understand spatial relationships and geometric transformations, leading to features that capture object structure and orientation - useful for many vision tasks.\"\n",
    ")\n",
    "\n",
    "mcq1.display()\n",
    "\n",
    "# Uncomment to check your answer:\n",
    "# mcq1.check_answer(2)  # Replace 2 with your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Time Series SSL - Autoencoder (30 minutes)\n",
    "\n",
    "Now let's explore **generative SSL** using autoencoders on time series data.\n",
    "\n",
    "## The Big Picture\n",
    "1. **Generate Data**: Create synthetic sine waves with different frequencies\n",
    "2. **Pretext Task**: Train autoencoder to reconstruct sequences (unsupervised)\n",
    "3. **Feature Learning**: Extract compressed representations from encoder\n",
    "4. **Transfer Learning**: Use embeddings for frequency classification\n",
    "5. **Analysis**: Compare autoencoder features vs raw sequences\n",
    "\n",
    "## Why Autoencoders Work for SSL\n",
    "- **Compression Forces Learning**: Bottleneck forces model to capture essential features\n",
    "- **Reconstruction Objective**: Learn to preserve important signal characteristics\n",
    "- **Unsupervised**: No labels needed for the pretext task\n",
    "- **Denoising Effect**: Can learn robust representations from noisy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Exercise 6: Synthetic Time Series Generation (5 minutes)\n",
    "\n",
    "First, let's create our synthetic time series dataset.\n",
    "\n",
    "**Your Task**: Implement a function to generate sine wave sequences with different frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sine_sequences(n_samples=1000, length=50, freq0=1.0, freq1=3.0, noise_std=0.1):\n",
    "    \"\"\"\n",
    "    Generate synthetic sine wave sequences with two different frequencies.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Total number of sequences to generate\n",
    "        length: Length of each sequence (time steps)\n",
    "        freq0: Frequency for class 0 (lower frequency)\n",
    "        freq1: Frequency for class 1 (higher frequency) \n",
    "        noise_std: Standard deviation of Gaussian noise to add\n",
    "    \n",
    "    Returns:\n",
    "        X: Array of sequences, shape (n_samples, length)\n",
    "        y: Array of frequency labels, shape (n_samples,)\n",
    "    \"\"\"\n",
    "    # TODO: Create time vector from 0 to 2œÄ\n",
    "    t = np.linspace(___) # FILL: (0, 2 * np.pi, length)\n",
    "    \n",
    "    # Split samples between two classes\n",
    "    half = n_samples // 2\n",
    "    \n",
    "    # TODO: Generate class 0 sequences (low frequency)\n",
    "    seq0 = np.sin(___) # FILL: freq0 * t\n",
    "    seq0 = np.tile(seq0, (half, 1))  # Repeat for half the samples\n",
    "    \n",
    "    # TODO: Generate class 1 sequences (high frequency)\n",
    "    seq1 = np.sin(___) # FILL: freq1 * t\n",
    "    seq1 = np.tile(seq1, (n_samples - half, 1))  # Repeat for remaining samples\n",
    "    \n",
    "    # Combine sequences\n",
    "    X = np.concatenate([seq0, seq1], axis=0)\n",
    "    \n",
    "    # TODO: Add Gaussian noise\n",
    "    X += np.random.normal(___) # FILL: scale=noise_std, size=X.shape\n",
    "    \n",
    "    # Create labels\n",
    "    y = np.concatenate([\n",
    "        np.zeros(half, dtype=int),  # Class 0 \n",
    "        np.ones(n_samples - half, dtype=int)  # Class 1\n",
    "    ])\n",
    "    \n",
    "    return X.astype(np.float32), y\n",
    "\n",
    "# TODO: Generate test data\n",
    "print(\"üåä Generating synthetic time series...\")\n",
    "# X_ts, y_ts = generate_sine_sequences(n_samples=1000, length=50, freq0=1.0, freq1=3.0, noise_std=0.1)\n",
    "# print(f\"üìä Time series data: {X_ts.shape}\")\n",
    "# print(f\"üè∑Ô∏è Class distribution: {np.bincount(y_ts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution and visualization\n",
    "def generate_sine_sequences_solution(n_samples=1000, length=50, freq0=1.0, freq1=3.0, noise_std=0.1):\n",
    "    t = np.linspace(0, 2 * np.pi, length)\n",
    "    half = n_samples // 2\n",
    "    \n",
    "    seq0 = np.sin(freq0 * t)\n",
    "    seq0 = np.tile(seq0, (half, 1))\n",
    "    \n",
    "    seq1 = np.sin(freq1 * t)\n",
    "    seq1 = np.tile(seq1, (n_samples - half, 1))\n",
    "    \n",
    "    X = np.concatenate([seq0, seq1], axis=0)\n",
    "    X += np.random.normal(scale=noise_std, size=X.shape)\n",
    "    \n",
    "    y = np.concatenate([\n",
    "        np.zeros(half, dtype=int),\n",
    "        np.ones(n_samples - half, dtype=int)\n",
    "    ])\n",
    "    \n",
    "    return X.astype(np.float32), y\n",
    "\n",
    "# Generate data\n",
    "X_ts, y_ts = generate_sine_sequences_solution(n_samples=1000, length=50, freq0=1.0, freq1=3.0, noise_std=0.1)\n",
    "print(f\"üåä Generated time series: {X_ts.shape}\")\n",
    "print(f\"üè∑Ô∏è Class 0 (freq=1.0): {np.sum(y_ts == 0)} samples\")\n",
    "print(f\"üè∑Ô∏è Class 1 (freq=3.0): {np.sum(y_ts == 1)} samples\")\n",
    "\n",
    "# Visualize examples from each class\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for i in range(3):\n",
    "    plt.plot(X_ts[i], alpha=0.7, label=f\"Sample {i+1}\")\n",
    "plt.title(f\"Class 0: Low Frequency (freq={1.0})\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(3):\n",
    "    idx = len(X_ts) - i - 1  # Get samples from class 1\n",
    "    plt.plot(X_ts[idx], alpha=0.7, label=f\"Sample {i+1}\")\n",
    "plt.title(f\"Class 1: High Frequency (freq={3.0})\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Exercise 7: Implement Autoencoder (15 minutes)\n",
    "\n",
    "Now let's implement a simple autoencoder for sequence reconstruction.\n",
    "\n",
    "**Your Task**: Complete the autoencoder implementation, focusing on the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "    def compute_loss(self, X, recon):\n        \"\"\"\n        Compute mean squared error reconstruction loss.\n        \n        MSE measures how well the autoencoder reconstructs its input:\n        - Perfect reconstruction: MSE = 0\n        - Poor reconstruction: MSE is large\n        - MSE = (1/n) * sum((original - reconstructed)¬≤)\n        \n        Args:\n            X: Original input sequences, shape (batch_size, sequence_length)\n            recon: Reconstructed sequences, shape (batch_size, sequence_length)\n            \n        Returns:\n            loss: Scalar MSE loss value\n        \"\"\"\n        # TODO: Implement MSE loss\n        # MSE computes the average squared difference between original and reconstruction\n        # np.mean() averages over all elements (both batch and sequence dimensions)\n        # Hint: MSE = mean((X - recon)^2)\n        loss = np.mean((X - recon)**2)  # FILL: np.mean((X - recon)**2)\n        return loss\n    \n    def backward(self, X, h, recon):\n        \"\"\"\n        Compute gradients for autoencoder parameters using backpropagation.\n        \n        This implements the chain rule for an autoencoder:\n        Loss = MSE(X, recon) where recon = decode(encode(X))\n        \n        Forward path: X ‚Üí encode ‚Üí h ‚Üí decode ‚Üí recon ‚Üí loss\n        Backward path: d_loss ‚Üê d_recon ‚Üê dh ‚Üê d_encoder ‚Üê d_X\n        \n        Args:\n            X: Original input, shape (batch_size, input_dim)\n            h: Hidden representation, shape (batch_size, hidden_dim) \n            recon: Reconstruction, shape (batch_size, input_dim)\n            \n        Returns:\n            Gradients for all parameters: dW_enc, db_enc, dW_dec, db_dec\n        \"\"\"\n        n_samples = X.shape[0]\n        \n        # TODO: Gradient of loss w.r.t. reconstruction\n        # MSE loss: L = (1/n) * sum((recon - X)¬≤)\n        # d_L/d_recon = (2/n) * (recon - X)\n        # Hint: For MSE loss, d_loss/d_recon = 2 * (recon - X) / n_samples\n        d_recon = 2 * (recon - X) / n_samples  # FILL: 2 * (recon - X) / n_samples\n        \n        # TODO: Decoder gradients\n        # Decoder: recon = h @ W_dec + b_dec\n        # d_L/d_W_dec = h.T @ d_recon  (matrix multiplication rule)\n        # d_L/d_b_dec = sum(d_recon, axis=0)  (bias gradients sum over batch)\n        # Hint: recon = h @ W_dec + b_dec\n        dW_dec = h.T @ d_recon  # FILL: h.T @ d_recon\n        db_dec = d_recon.sum(axis=0)  # FILL: d_recon.sum(axis=0)\n        \n        # TODO: Propagate gradients to hidden layer\n        # Chain rule: d_L/d_h = d_recon @ W_dec.T\n        # This flows gradients from decoder back to encoder\n        # Hint: Chain rule through decoder weights\n        dh = d_recon @ self.W_dec.T  # FILL: d_recon @ self.W_dec.T\n        \n        # TODO: Gradient through tanh activation\n        # Encoder uses tanh activation: h = tanh(z)\n        # d_tanh/d_z = 1 - tanh¬≤(z) = 1 - h¬≤ (since h = tanh(z))\n        # Hint: d_tanh/d_z = 1 - tanh^2(z)\n        dz = dh * (1.0 - h**2)  # Element-wise multiplication\n        \n        # TODO: Encoder gradients  \n        # Encoder: z = X @ W_enc + b_enc\n        # d_L/d_W_enc = X.T @ dz  (matrix multiplication rule)\n        # d_L/d_b_enc = sum(dz, axis=0)  (bias gradients sum over batch)\n        # Hint: z = X @ W_enc + b_enc\n        dW_enc = X.T @ dz  # FILL: X.T @ dz\n        db_enc = dz.sum(axis=0)  # FILL: dz.sum(axis=0)\n        \n        return dW_enc, db_enc, dW_dec, db_dec"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Test Your Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test autoencoder initialization and forward pass\n",
    "print(\"üß™ Testing autoencoder implementation...\")\n",
    "\n",
    "# Create small test autoencoder\n",
    "test_ae = Autoencoder(input_dim=50, hidden_dim=16, learning_rate=0.05)\n",
    "print(f\"‚úÖ Autoencoder created: {test_ae.input_dim} ‚Üí {test_ae.hidden_dim} ‚Üí {test_ae.input_dim}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_data = X_ts[:5]  # 5 test sequences\n",
    "h, recon = test_ae.forward(test_data)\n",
    "print(f\"‚úÖ Forward pass successful!\")\n",
    "print(f\"   Input shape: {test_data.shape}\")\n",
    "print(f\"   Hidden shape: {h.shape}\")\n",
    "print(f\"   Reconstruction shape: {recon.shape}\")\n",
    "\n",
    "# Test loss computation\n",
    "loss = test_ae.compute_loss(test_data, recon)\n",
    "print(f\"‚úÖ Loss computation: {loss:.6f}\")\n",
    "\n",
    "# TODO: Uncomment to test your backward pass implementation\n",
    "# grads = test_ae.backward(test_data, h, recon)\n",
    "# print(f\"‚úÖ Backward pass successful!\")\n",
    "# print(f\"   Gradient shapes: dW_enc={grads[0].shape}, dW_dec={grads[2].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for backward pass - complete implementation\n",
    "class AutoencoderComplete(Autoencoder):\n",
    "    def compute_loss(self, X, recon):\n",
    "        return np.mean((X - recon)**2)\n",
    "    \n",
    "    def backward(self, X, h, recon):\n",
    "        n_samples = X.shape[0]\n",
    "        d_recon = 2 * (recon - X) / n_samples\n",
    "        dW_dec = h.T @ d_recon\n",
    "        db_dec = d_recon.sum(axis=0)\n",
    "        dh = d_recon @ self.W_dec.T\n",
    "        dz = dh * (1.0 - h**2)\n",
    "        dW_enc = X.T @ dz\n",
    "        db_enc = dz.sum(axis=0)\n",
    "        return dW_enc, db_enc, dW_dec, db_dec\n",
    "\n",
    "# Test complete implementation\n",
    "test_ae_complete = AutoencoderComplete(input_dim=50, hidden_dim=16, learning_rate=0.05)\n",
    "h, recon = test_ae_complete.forward(test_data)\n",
    "grads = test_ae_complete.backward(test_data, h, recon)\n",
    "print(f\"‚úÖ Complete autoencoder backward pass successful!\")\n",
    "print(f\"   All gradient shapes correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Exercise 8: Train Autoencoder and Analyze Results (10 minutes)\n",
    "\n",
    "Now let's train the autoencoder and visualize the reconstruction quality.\n",
    "\n",
    "**Your Task**: Set up training and analyze the learned representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare time series data for training\n",
    "print(\"üìä Preparing time series data...\")\n",
    "\n",
    "# TODO: Split time series data\n",
    "X_train_ts, X_test_ts, y_train_ts, y_test_ts = train_test_split(\n",
    "    ___,  # FILL: X_ts\n",
    "    ___,  # FILL: y_ts\n",
    "    test_size=0.3,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# TODO: Normalize sequences (important for training stability)\n",
    "# Normalize to zero mean and unit variance per sequence\n",
    "X_train_norm = (X_train_ts - X_train_ts.mean(axis=1, keepdims=True)) / (X_train_ts.std(axis=1, keepdims=True) + 1e-6)\n",
    "X_test_norm = (X_test_ts - X_test_ts.mean(axis=1, keepdims=True)) / (X_test_ts.std(axis=1, keepdims=True) + 1e-6)\n",
    "\n",
    "print(f\"üìä Training set: {X_train_norm.shape}\")\n",
    "print(f\"üìä Test set: {X_test_norm.shape}\")\n",
    "print(f\"üìä Normalized data range: [{X_train_norm.min():.2f}, {X_train_norm.max():.2f}]\")\n",
    "\n",
    "# TODO: Create and configure autoencoder\n",
    "# Hint: input_dim=sequence_length, hidden_dim=16 for compression\n",
    "ae = AutoencoderComplete(\n",
    "    input_dim=___,  # FILL: X_train_norm.shape[1]\n",
    "    hidden_dim=___,  # FILL: 16\n",
    "    learning_rate=0.05\n",
    ")\n",
    "\n",
    "print(f\"üß† Autoencoder: {ae.input_dim} ‚Üí {ae.hidden_dim} ‚Üí {ae.input_dim}\")\n",
    "print(f\"üìâ Compression ratio: {ae.input_dim/ae.hidden_dim:.1f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution and training\n",
    "X_train_ts, X_test_ts, y_train_ts, y_test_ts = train_test_split(\n",
    "    X_ts, y_ts, test_size=0.3, random_state=0\n",
    ")\n",
    "\n",
    "X_train_norm = (X_train_ts - X_train_ts.mean(axis=1, keepdims=True)) / (X_train_ts.std(axis=1, keepdims=True) + 1e-6)\n",
    "X_test_norm = (X_test_ts - X_test_ts.mean(axis=1, keepdims=True)) / (X_test_ts.std(axis=1, keepdims=True) + 1e-6)\n",
    "\n",
    "ae = AutoencoderComplete(input_dim=X_train_norm.shape[1], hidden_dim=16, learning_rate=0.05)\n",
    "\n",
    "print(\"üöÄ Training autoencoder...\")\n",
    "ae_losses = ae.train(X_train_norm, epochs=30, batch_size=128, verbose=True)\n",
    "\n",
    "# Plot training curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(ae_losses, 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Autoencoder Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')  # Log scale to see improvement clearly\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Training completed!\")\n",
    "print(f\"   Initial loss: {ae_losses[0]:.6f}\")\n",
    "print(f\"   Final loss: {ae_losses[-1]:.6f}\")\n",
    "print(f\"   Improvement: {(ae_losses[0] - ae_losses[-1])/ae_losses[0]*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Visualize Reconstruction Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze reconstruction quality\n",
    "print(\"üîç Analyzing reconstruction quality...\")\n",
    "\n",
    "# Select examples from test set\n",
    "n_examples = 6\n",
    "example_indices = np.random.choice(len(X_test_norm), n_examples, replace=False)\n",
    "\n",
    "# Get reconstructions\n",
    "test_samples = X_test_norm[example_indices]\n",
    "reconstructions = ae.reconstruct(test_samples)\n",
    "\n",
    "# Plot original vs reconstructed\n",
    "fig, axes = plt.subplots(n_examples, 2, figsize=(12, 2*n_examples))\n",
    "\n",
    "for i in range(n_examples):\n",
    "    idx = example_indices[i]\n",
    "    original = test_samples[i]\n",
    "    reconstructed = reconstructions[i]\n",
    "    \n",
    "    # Original sequence\n",
    "    axes[i, 0].plot(original, 'b-', linewidth=2, label='Original')\n",
    "    axes[i, 0].plot(reconstructed, 'r--', linewidth=2, label='Reconstructed')\n",
    "    axes[i, 0].set_ylabel(f\"Seq {idx}\\n(Class {y_test_ts[idx]})\")\n",
    "    axes[i, 0].legend()\n",
    "    axes[i, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Reconstruction error\n",
    "    error = original - reconstructed\n",
    "    mse = np.mean(error**2)\n",
    "    axes[i, 1].plot(error, 'g-', linewidth=2)\n",
    "    axes[i, 1].set_ylabel(f\"Error\\n(MSE: {mse:.4f})\")\n",
    "    axes[i, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 0].set_title(\"Original vs Reconstructed\")\n",
    "axes[0, 1].set_title(\"Reconstruction Error\")\n",
    "axes[-1, 0].set_xlabel(\"Time Step\")\n",
    "axes[-1, 1].set_xlabel(\"Time Step\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Overall reconstruction statistics\n",
    "all_reconstructions = ae.reconstruct(X_test_norm)\n",
    "mse_per_sample = np.mean((X_test_norm - all_reconstructions)**2, axis=1)\n",
    "print(f\"üìä Reconstruction Quality Statistics:\")\n",
    "print(f\"   Mean MSE: {mse_per_sample.mean():.6f}\")\n",
    "print(f\"   Std MSE: {mse_per_sample.std():.6f}\")\n",
    "print(f\"   Best reconstruction MSE: {mse_per_sample.min():.6f}\")\n",
    "print(f\"   Worst reconstruction MSE: {mse_per_sample.max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Exercise 9: Transfer Learning with Embeddings (10 minutes)\n",
    "\n",
    "Now let's use the learned representations for frequency classification.\n",
    "\n",
    "**Your Task**: Compare classification performance using autoencoder embeddings vs raw sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Extracting embeddings for classification...\")\n",
    "\n",
    "# TODO: Extract embeddings using the encoder\n",
    "train_embeddings = ___  # FILL: ae.encode(X_train_norm)\n",
    "test_embeddings = ___  # FILL: ae.encode(X_test_norm)\n",
    "\n",
    "print(f\"üìä Embeddings shape: {train_embeddings.shape}\")\n",
    "print(f\"üìâ Dimensionality reduction: {X_train_norm.shape[1]} ‚Üí {train_embeddings.shape[1]}\")\n",
    "\n",
    "# TODO: Train classifier on embeddings\n",
    "print(\"\\nüöÄ Training classifiers...\")\n",
    "clf_embeddings = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf_embeddings.fit(___) # FILL: train_embeddings, y_train_ts\n",
    "emb_acc = clf_embeddings.score(___) # FILL: test_embeddings, y_test_ts\n",
    "\n",
    "# TODO: Train baseline classifier on raw normalized sequences\n",
    "clf_raw_ts = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf_raw_ts.fit(___) # FILL: X_train_norm, y_train_ts\n",
    "raw_acc = clf_raw_ts.score(___) # FILL: X_test_norm, y_test_ts\n",
    "\n",
    "print(f\"\\nüìä Time Series Classification Results:\")\n",
    "print(f\"   üß† Autoencoder Embeddings: {emb_acc:.3f}\")\n",
    "print(f\"   üìà Raw Sequences: {raw_acc:.3f}\")\n",
    "print(f\"   üìä Difference: {(emb_acc - raw_acc)*100:+.1f} percentage points\")\n",
    "\n",
    "if emb_acc >= raw_acc - 0.02:  # Within 2 percentage points\n",
    "    print(f\"   ‚úÖ Embeddings retain discriminative information despite compression!\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Some discriminative information lost in compression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "train_embeddings = ae.encode(X_train_norm)\n",
    "test_embeddings = ae.encode(X_test_norm)\n",
    "\n",
    "clf_embeddings = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf_embeddings.fit(train_embeddings, y_train_ts)\n",
    "emb_acc = clf_embeddings.score(test_embeddings, y_test_ts)\n",
    "\n",
    "clf_raw_ts = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf_raw_ts.fit(X_train_norm, y_train_ts)\n",
    "raw_acc = clf_raw_ts.score(X_test_norm, y_test_ts)\n",
    "\n",
    "print(f\"üìä Time Series Classification Results:\")\n",
    "print(f\"   üß† Autoencoder Embeddings: {emb_acc:.3f}\")\n",
    "print(f\"   üìà Raw Sequences: {raw_acc:.3f}\")\n",
    "print(f\"   üî• Compression: {X_train_norm.shape[1]} ‚Üí {train_embeddings.shape[1]} dimensions\")\n",
    "\n",
    "# Visualize embeddings with t-SNE\n",
    "print(\"\\nüé® Visualizing embedding space...\")\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(test_embeddings[:300])  # Subset for speed\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['blue', 'red']\n",
    "labels = ['Low Freq (1.0)', 'High Freq (3.0)']\n",
    "\n",
    "for class_id in [0, 1]:\n",
    "    mask = y_test_ts[:300] == class_id\n",
    "    plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], \n",
    "               c=colors[class_id], alpha=0.6, s=30, label=labels[class_id])\n",
    "\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.title('Autoencoder Embeddings Visualization (t-SNE)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "if emb_acc >= raw_acc - 0.02:\n",
    "    print(\"‚úÖ Autoencoder successfully learned discriminative features!\")\n",
    "else:\n",
    "    print(\"üìù Note: Perfect separation isn't always expected - depends on noise level and frequencies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Multiple Choice Question 2\n",
    "**What is the main benefit of using an autoencoder for self-supervised learning?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcq2 = MultipleChoiceQuestion(\n",
    "    \"What is the main benefit of using an autoencoder for self-supervised learning?\",\n",
    "    [\n",
    "        \"It always produces better accuracy than supervised learning\",\n",
    "        \"It compresses data into a lower-dimensional representation while preserving important features\",\n",
    "        \"It can only work with time series data\",\n",
    "        \"It requires less computation than other SSL methods\"\n",
    "    ],\n",
    "    2,\n",
    "    \"The autoencoder's bottleneck architecture forces it to learn compressed representations that capture the most important aspects of the data, making it useful for dimensionality reduction and feature learning.\"\n",
    ")\n",
    "\n",
    "mcq2.display()\n",
    "# mcq2.check_answer(2)  # Replace with your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Advanced Concepts & Extensions (15 minutes)\n",
    "\n",
    "Let's explore more advanced SSL concepts and give you a chance to be creative!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Critical Thinking Question 3\n",
    "\n",
    "**We've seen two SSL approaches: rotation prediction (discriminative) and autoencoders (generative). What are the trade-offs between these approaches? When would you choose one over the other?**\n",
    "\n",
    "*Think about: computational efficiency, data requirements, robustness, and the types of features learned.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Exercise 10: Design a Novel Pretext Task (Creative Challenge)\n",
    "\n",
    "Now it's your turn to be creative! Design a novel pretext task for a domain of your choice.\n",
    "\n",
    "**Your Task**: Choose a data type and design a self-supervised pretext task. Implement a basic version if you have time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for your creative pretext task\n",
    "\n",
    "print(\"üé® Creative Exercise: Design Your Own Pretext Task\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nüí° Instructions:\")\n",
    "print(\"1. Choose a data domain (text, audio, images, graphs, etc.)\")\n",
    "print(\"2. Design a pretext task where labels are automatically generated\")\n",
    "print(\"3. Explain why this task would learn useful features\")\n",
    "print(\"4. [Optional] Implement a basic version\")\n",
    "print(\"\\nüåü Example ideas:\")\n",
    "print(\"   ‚Ä¢ Text: Predict if two sentences are consecutive in a document\")\n",
    "print(\"   ‚Ä¢ Audio: Predict the temporal order of audio segments\")\n",
    "print(\"   ‚Ä¢ Images: Predict relative spatial positions of image patches\")\n",
    "print(\"   ‚Ä¢ Graphs: Predict if two nodes are connected\")\n",
    "print(\"   ‚Ä¢ Video: Predict the speed of video playback\")\n",
    "\n",
    "# TODO: Describe your pretext task here\n",
    "your_pretext_task = \"\"\"\n",
    "TODO: Fill in your creative pretext task design\n",
    "\n",
    "Data Domain: [e.g., text, images, audio, etc.]\n",
    "\n",
    "Pretext Task: [Describe what the model needs to predict]\n",
    "\n",
    "Label Generation: [How are labels created automatically?]\n",
    "\n",
    "Why It Works: [What useful features would this task learn?]\n",
    "\n",
    "Potential Applications: [What downstream tasks could benefit?]\n",
    "\n",
    "Implementation Challenges: [What would be tricky to implement?]\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nüìù Your Pretext Task Design:\")\n",
    "print(your_pretext_task)\n",
    "\n",
    "# TODO: [Optional] Implement a basic version of your pretext task\n",
    "def your_pretext_task_implementation():\n",
    "    \"\"\"\n",
    "    Optional: Implement a basic version of your pretext task.\n",
    "    This could be data generation, a simple model, or just pseudocode.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "print(\"\\nüí≠ Reflection Questions:\")\n",
    "print(\"   ‚Ä¢ Is your task discriminative or generative?\")\n",
    "print(\"   ‚Ä¢ How difficult would it be to implement at scale?\")\n",
    "print(\"   ‚Ä¢ What domains could this transfer to?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Advanced SSL Concepts (Summary)\n",
    "\n",
    "Here are some cutting-edge SSL approaches you might explore further:\n",
    "\n",
    "### üîÑ Contrastive Learning\n",
    "- **SimCLR**: Learn representations by maximizing agreement between different augmentations\n",
    "- **MoCo**: Maintain a momentum-updated queue of negative examples\n",
    "- **SwAV**: Use clustering assignments as pseudo-labels\n",
    "\n",
    "### üé≠ Masked Modeling\n",
    "- **BERT**: Mask words in sentences and predict them\n",
    "- **MAE**: Mask image patches and reconstruct them\n",
    "- **GraphMAE**: Mask nodes/edges in graphs\n",
    "\n",
    "### üîÄ Multi-Modal SSL\n",
    "- **CLIP**: Learn joint text-image representations\n",
    "- **ALIGN**: Scale up text-image learning with noisy data\n",
    "- **AudioCLIP**: Extend to audio-visual learning\n",
    "\n",
    "### üéØ Domain-Specific SSL\n",
    "- **Protein folding**: Predict 3D structure from sequence\n",
    "- **Medical imaging**: Anatomical consistency checks\n",
    "- **Robotics**: Predict future states from actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Assessment & Reflection (15 minutes)\n",
    "\n",
    "Time to test your understanding with AI-powered evaluation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ AI-Powered Open-Ended Assessment\n",
    "\n",
    "These questions will be evaluated automatically using Claude API if you have it configured.\n",
    "\n",
    "### Setting up Assessment\n",
    "Make sure you have `MY_APP_ANTHROPIC_KEY` set in your environment for automatic evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessment system setup (same as before)\n",
    "import os\n",
    "from typing import Dict, List, Optional\n",
    "import json\n",
    "\n",
    "class OpenEndedAssessment:\n",
    "    \"\"\"Handle open-ended questions with Claude AI verification.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: Optional[str] = None):\n",
    "        self.api_key = api_key or os.getenv('MY_APP_ANTHROPIC_KEY')\n",
    "        self.questions = self._load_questions()\n",
    "        \n",
    "        if self.api_key:\n",
    "            try:\n",
    "                import anthropic\n",
    "                self.client = anthropic.Anthropic(api_key=self.api_key)\n",
    "                print(\"‚úÖ Claude API configured for automatic evaluation\")\n",
    "            except ImportError:\n",
    "                print(\"‚ö†Ô∏è Please install anthropic: pip install anthropic\")\n",
    "                self.client = None\n",
    "        else:\n",
    "            self.client = None\n",
    "            print(\"‚ÑπÔ∏è No API key found - will use manual evaluation mode\")\n",
    "    \n",
    "    def _load_questions(self) -> List[Dict]:\n",
    "        return [\n",
    "            {\n",
    "                \"id\": \"q1\",\n",
    "                \"question\": \"Explain why rotation prediction is an effective pretext task for learning visual features. What properties of the task make it useful for downstream computer vision tasks?\",\n",
    "                \"rubric\": [\n",
    "                    \"Explains that rotation requires understanding geometric and spatial structure\",\n",
    "                    \"Notes that labels are automatically generated (no human annotation needed)\", \n",
    "                    \"Discusses how it forces learning of rotation-equivariant or invariant features\",\n",
    "                    \"Connects to usefulness for downstream tasks requiring spatial understanding\"\n",
    "                ],\n",
    "                \"sample_answer\": \"Rotation prediction is effective because it forces networks to understand spatial relationships and geometric structure in images. The task requires recognizing objects regardless of orientation, leading to rotation-equivariant representations. Labels are free since we can rotate any image and know the rotation angle. The learned features capture shape, structure, and spatial patterns that transfer well to classification, detection, and other vision tasks.\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"q2\", \n",
    "                \"question\": \"Compare and contrast autoencoders with contrastive learning methods for self-supervised learning. What are the strengths and weaknesses of each approach?\",\n",
    "                \"rubric\": [\n",
    "                    \"Identifies autoencoders as generative/reconstructive approach\",\n",
    "                    \"Identifies contrastive learning as discriminative approach\", \n",
    "                    \"Discusses computational and data efficiency differences\",\n",
    "                    \"Explains when to choose one approach over the other\"\n",
    "                ],\n",
    "                \"sample_answer\": \"Autoencoders learn by reconstructing inputs, capturing detailed information but potentially including noise. They're computationally simpler and work well for dimensionality reduction. Contrastive methods learn by comparing examples, focusing on discriminative features while ignoring irrelevant details. Contrastive approaches are more robust to noise and often learn better features for classification, but require careful augmentation strategies and can be computationally expensive. Choose autoencoders for compression and reconstruction tasks, contrastive methods for discriminative downstream tasks.\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"q3\",\n",
    "                \"question\": \"Design a novel self-supervised pretext task for a domain of your choice (not vision or time series). Explain your reasoning and why it would learn useful features.\",\n",
    "                \"rubric\": [\n",
    "                    \"Proposes a specific, well-defined pretext task for a clear domain\",\n",
    "                    \"Explains how supervision signals would be generated automatically\", \n",
    "                    \"Provides clear reasoning for why the task would learn useful features\",\n",
    "                    \"Discusses potential applications and transfer learning scenarios\"\n",
    "                ],\n",
    "                \"sample_answer\": \"For natural language processing, I propose 'discourse coherence prediction': given two paragraphs, predict whether they appear consecutively in the same document. This requires understanding topic flow, argument structure, and linguistic coherence markers. Labels are free since any document provides positive examples (consecutive paragraphs) and negative examples (random pairs). The model would learn discourse-level features useful for document classification, summarization, and text generation tasks.\"\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def evaluate_answer(self, question_id: str, user_answer: str) -> Dict:\n",
    "        question_data = next((q for q in self.questions if q['id'] == question_id), None)\n",
    "        if not question_data:\n",
    "            return {\"error\": \"Question not found\"}\n",
    "        \n",
    "        if not self.client:\n",
    "            return self._manual_evaluation(question_data, user_answer)\n",
    "        \n",
    "        try:\n",
    "            evaluation_prompt = self._create_evaluation_prompt(question_data, user_answer)\n",
    "            response = self._call_claude_api(evaluation_prompt)\n",
    "            return self._parse_evaluation(response)\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"API call failed: {str(e)}\"}\n",
    "    \n",
    "    def _create_evaluation_prompt(self, question_data: Dict, user_answer: str) -> str:\n",
    "        prompt = f\"\"\"You are evaluating a student's understanding of self-supervised learning.\n",
    "\n",
    "Question: {question_data['question']}\n",
    "\n",
    "Evaluation Rubric (25 points each):\n",
    "{chr(10).join(f'- {item}' for item in question_data['rubric'])}\n",
    "\n",
    "Reference Answer: {question_data['sample_answer']}\n",
    "\n",
    "Student Answer: {user_answer}\n",
    "\n",
    "Please evaluate and provide a JSON response:\n",
    "{{\n",
    "    \"score\": <0-100>,\n",
    "    \"rubric_met\": [<addressed rubric points>],\n",
    "    \"strengths\": \"<what was done well>\",\n",
    "    \"improvements\": \"<areas for improvement>\",\n",
    "    \"feedback\": \"<encouraging constructive feedback>\"\n",
    "}}\n",
    "\n",
    "Be encouraging while providing honest evaluation. Focus on conceptual understanding.\n",
    "Return ONLY the JSON object.\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def _call_claude_api(self, prompt: str) -> str:\n",
    "        message = self.client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=500,\n",
    "            temperature=0.3,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return message.content[0].text\n",
    "    \n",
    "    def _parse_evaluation(self, response: str) -> Dict:\n",
    "        try:\n",
    "            import re\n",
    "            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "            if json_match:\n",
    "                return json.loads(json_match.group())\n",
    "            return json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"error\": \"Failed to parse response\", \"raw_response\": response}\n",
    "    \n",
    "    def _manual_evaluation(self, question_data: Dict, user_answer: str) -> Dict:\n",
    "        return {\n",
    "            \"message\": \"Manual evaluation mode - use the rubric below to self-assess\",\n",
    "            \"rubric\": question_data['rubric'], \n",
    "            \"sample_answer\": question_data['sample_answer'],\n",
    "            \"your_answer\": user_answer,\n",
    "            \"instructions\": \"Compare your answer to the sample and rubric. Award yourself points for each addressed rubric item.\"\n",
    "        }\n",
    "\n",
    "def submit_answer(question_id: str, answer: str):\n",
    "    print(f\"\\nüìä Evaluating Question {question_id}...\\n\")\n",
    "    result = assessment.evaluate_answer(question_id, answer)\n",
    "    \n",
    "    if 'error' in result:\n",
    "        print(f\"‚ùå Error: {result['error']}\")\n",
    "    elif 'message' in result:\n",
    "        print(f\"‚ÑπÔ∏è {result['message']}\\n\")\n",
    "        print(\"üìã Rubric (25 points each):\")\n",
    "        for i, item in enumerate(result['rubric'], 1):\n",
    "            print(f\"  {i}. {item}\")\n",
    "        print(f\"\\nüìñ Sample Answer: {result['sample_answer']}\")\n",
    "        print(f\"\\n‚úçÔ∏è Your Answer: {result['your_answer']}\")\n",
    "    else:\n",
    "        print(f\"üéØ Score: {result['score']}/100\\n\")\n",
    "        if 'rubric_met' in result:\n",
    "            print(\"‚úÖ Rubric Points Addressed:\")\n",
    "            for point in result['rubric_met']:\n",
    "                print(f\"  ‚Ä¢ {point}\")\n",
    "        print(f\"\\nüí™ Strengths: {result.get('strengths', 'N/A')}\")\n",
    "        print(f\"\\nüìà Areas for Improvement: {result.get('improvements', 'N/A')}\")\n",
    "        print(f\"\\nüí¨ Feedback: {result.get('feedback', 'N/A')}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Initialize assessment\n",
    "assessment = OpenEndedAssessment()\n",
    "print(\"\\nüìù Assessment system ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Question 1: Rotation Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ùì Question 1: Rotation Prediction Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Explain why rotation prediction is an effective pretext task for learning visual features.\")\n",
    "print(\"What properties of the task make it useful for downstream computer vision tasks?\")\n",
    "print(\"\\nüí° Think about: geometric understanding, label generation, learned features, transfer learning\")\n",
    "\n",
    "# TODO: Write your answer here\n",
    "my_answer_q1 = \"\"\"\n",
    "Write your thoughtful answer here...\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Uncomment to submit your answer\n",
    "# result_q1 = submit_answer(\"q1\", my_answer_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Question 2: Autoencoders vs Contrastive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ùì Question 2: Method Comparison\")\n",
    "print(\"=\" * 50) \n",
    "print(\"Compare and contrast autoencoders with contrastive learning methods.\")\n",
    "print(\"What are the strengths and weaknesses of each approach?\")\n",
    "print(\"\\nüí° Think about: learning objectives, computational costs, robustness, use cases\")\n",
    "\n",
    "# TODO: Write your answer here\n",
    "my_answer_q2 = \"\"\"\n",
    "Write your comparative analysis here...\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Uncomment to submit your answer  \n",
    "# result_q2 = submit_answer(\"q2\", my_answer_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Question 3: Creative Pretext Task Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ùì Question 3: Creative Design Challenge\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Design a novel self-supervised pretext task for a domain of your choice.\")\n",
    "print(\"(Choose a domain other than computer vision or time series)\")\n",
    "print(\"\\nüí° Consider: text, audio, graphs, biology, chemistry, social networks, etc.\")\n",
    "print(\"\\nüìã Address: domain choice, task description, label generation, why it works, applications\")\n",
    "\n",
    "# TODO: Write your creative pretext task\n",
    "my_answer_q3 = \"\"\"\n",
    "Design your novel pretext task here...\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Uncomment to submit your answer\n",
    "# result_q3 = submit_answer(\"q3\", my_answer_q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üéâ Congratulations! Lab Complete!\n",
    "\n",
    "## üìö What You've Accomplished\n",
    "\n",
    "‚úÖ **Understood SSL Fundamentals**: Learned the difference between generative and discriminative approaches\n",
    "\n",
    "‚úÖ **Implemented Rotation Prediction**: Built a complete SSL system from data generation to transfer learning\n",
    "\n",
    "‚úÖ **Built Neural Networks from Scratch**: Implemented forward/backward propagation using only NumPy\n",
    "\n",
    "‚úÖ **Created Autoencoders**: Developed generative SSL for time series reconstruction\n",
    "\n",
    "‚úÖ **Applied Transfer Learning**: Used SSL features for downstream classification tasks\n",
    "\n",
    "‚úÖ **Analyzed Representations**: Visualized learned features and embedding spaces\n",
    "\n",
    "‚úÖ **Designed Novel Tasks**: Created your own pretext task for a new domain\n",
    "\n",
    "## üß† Key Insights\n",
    "\n",
    "- **SSL generates its own supervision** from data structure\n",
    "- **Pretext tasks must be challenging enough** to force meaningful feature learning\n",
    "- **Transfer learning reveals representation quality** through downstream performance\n",
    "- **Different SSL approaches** (generative vs discriminative) have different strengths\n",
    "- **Domain knowledge is crucial** for designing effective pretext tasks\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "### üìñ **Continue Learning**\n",
    "- Explore modern SSL methods: SimCLR, MoCo, MAE\n",
    "- Study domain-specific SSL applications\n",
    "- Learn about multi-modal SSL (CLIP, ALIGN)\n",
    "\n",
    "### üíª **Hands-On Projects**  \n",
    "- Implement SSL on your own datasets\n",
    "- Try SSL with real-world computer vision tasks\n",
    "- Experiment with different pretext tasks\n",
    "- Build SSL systems for text or audio\n",
    "\n",
    "### üî¨ **Research Directions**\n",
    "- Design pretext tasks for your domain\n",
    "- Investigate why SSL works theoretically\n",
    "- Combine multiple pretext tasks\n",
    "- Explore SSL for few-shot learning\n",
    "\n",
    "## üåü Final Reflection\n",
    "\n",
    "Self-supervised learning represents a paradigm shift in machine learning - leveraging the vast amounts of unlabeled data around us by finding clever ways to create supervision signals. The techniques you've learned here are actively used in cutting-edge AI systems, from language models to computer vision applications.\n",
    "\n",
    "The key to successful SSL is creativity: finding the right pretext task that forces models to learn the features you care about for downstream tasks. Keep experimenting, and remember that some of the best SSL ideas come from deep understanding of the data and domain!\n",
    "\n",
    "**Happy learning! üéì**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssnn-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}